# -*- coding: utf-8 -*-
import os
import threading
import pandas as pd
from huggingface_hub import InferenceClient
from ai_energy.utils.rag_utils import retrieve_context

# ğŸ” RÃ©cupÃ©ration du token dâ€™authentification
hf_token = os.getenv("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")  # VÃ©rifie quâ€™il est bien exportÃ©

# âœ… Client avec modÃ¨le validÃ©
client = InferenceClient(
    model="meta-llama/Meta-Llama-Guard-2-8B",
    token=hf_token
)

class TimeoutException(Exception):
    pass

def run_with_timeout(func, args=(), kwargs={}, timeout=180):
    result = {}
    def wrapper():
        try:
            result['value'] = func(*args, **kwargs)
        except Exception as e:
            result['error'] = e
    thread = threading.Thread(target=wrapper)
    thread.start()
    thread.join(timeout)
    if thread.is_alive():
        raise TimeoutException("â° Temps d'exÃ©cution dÃ©passÃ©.")
    if 'error' in result:
        raise result['error']
    return result.get('value')

def generer_rapport_anomalie(anomalies_df: pd.DataFrame):
    if anomalies_df.empty:
        return "âœ… Aucune anomalie dÃ©tectÃ©e dans les donnÃ©es Ã©nergÃ©tiques."

    prompt = (
        "Tu es un expert en efficacitÃ© Ã©nergÃ©tique industrielle.\n"
        "Voici une sÃ©rie d'anomalies dÃ©tectÃ©es dans une entreprise :\n"
    )

    for i, (_, row) in enumerate(anomalies_df.iterrows()):
        if i >= 10:
            break
        prompt += f"- ğŸ“… {row['date']} : erreur dÃ©tectÃ©e = {round(row['reconstruction_error'], 4)}\n"

    prompt += (
        "\nPour chaque date, donne :\n"
        "ğŸŸ¡ Une hypothÃ¨se sur la cause (ex : capteur dÃ©faillant, consommation anormale...)\n"
        "ğŸ› ï¸ Une recommandation concrÃ¨te pour rÃ©soudre ou prÃ©venir l'anomalie.\n"
        "Structure la rÃ©ponse par date. Utilise des paragraphes clairs, des emojis professionnels et un ton sÃ©rieux.\n"
    )

    # ğŸ“š Ajouter contexte RAG
    context = retrieve_context(prompt, top_k=3)
    final_prompt = f"ğŸ“˜ Contexte documentaire :\n{context}\n\nğŸ“© Question :\n{prompt}"

    print("ğŸ§  Prompt final envoyÃ© au LLM :\n", final_prompt[:500], "...\n")

    try:
        def generate():
            response = client.chat.completions.create(
                model="meta-llama/Meta-Llama-Guard-2-8B",
                messages=[
                    {"role": "user", "content": final_prompt}
                ],
                max_tokens=500,
                temperature=0.7
            )
            return response.choices[0].message.content.strip()

        return run_with_timeout(generate, timeout=180)

    except TimeoutException as e:
        return str(e)
    except Exception as e:
        return f"âŒ Erreur lors de la gÃ©nÃ©ration : {str(e)}"
